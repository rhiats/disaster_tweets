{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict whether or not a tweet is about a real disaster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "    \n",
    "    https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7219856"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview\n",
    "\n",
    "There are 7,613 training data points and 3,263 test datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_shape(filename):\n",
    "    \"\"\"\n",
    "        Return the shape (row,columns) of the dataset in the .csv file\n",
    "        \n",
    "        @P: filename (string) of a csv data set file\n",
    "        @R: tuple of dataframe size\n",
    "        \n",
    "    \"\"\"\n",
    "    df=pd.read_csv(filename)\n",
    "    return df.shape\n",
    "\n",
    "def hist_rel_freq(filename,columnName,mx_val):\n",
    "    \"\"\"\n",
    "        Save a relative frequency of a specific column with values that are numerical\n",
    "        \n",
    "        @P: filename: (csv) the training data file\n",
    "            columnName: (dtring) the name of the column of interest\n",
    "            mx_val: (int) the max range for the x-axis\n",
    "            \n",
    "    \"\"\"\n",
    "    df=pd.read_csv(filename)\n",
    "    fig, ax = plt.subplots()\n",
    "    g=sns.histplot(data=df, x=columnName, stat=\"percent\", discrete=True, ax=ax)\n",
    "    ax.set_xlim(-1,mx_val+1)\n",
    "    ax.set_xticks(range(0,mx_val+1))\n",
    "    plt.savefig('output/dist_{}.png'.format(columnName))\n",
    "    print(\"\\n Column: {} Relative Frequency \\n\".format(columnName),df[columnName].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training data is (7613, 5) \n",
      "The shape of the test data is (3263, 4) \n",
      "\n",
      " Column: target Relative Frequency \n",
      " 0    0.57034\n",
      "1    0.42966\n",
      "Name: target, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The shape of the training data is {} \".format(df_shape(\"input/train.csv\")))\n",
    "print(\"The shape of the test data is {} \".format(df_shape(\"input/test.csv\")))\n",
    "hist_rel_freq(\"input/train.csv\",\"target\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process\n",
    "\n",
    "• Convert all the words to lowercase\n",
    "\n",
    "• Lemmatize all thewords (i.e., convert every word to its root so that all of “running,” “run,” and “runs” are converted to “run” and and all of “good,” “well,” “better,” and “best” are converted to “good”; this is easily done using nltk.stem) - (CHOSE NOT TO LEMMATIZE, TO PRESERVE MEANING OF TEXT - CONSIDER WRITING MY OWN FUNCTION)\n",
    "\n",
    "• Strip punctuation\n",
    "\n",
    "• Strip the stop words, e.g., “the”, “and”, “or” (ADDITIONAL STOP WORDS?)\n",
    "\n",
    "• Strip @ and urls (It’s Twitter.)\n",
    "\n",
    "• Something else? Tell us about it (SPELL CHECK, Repeated letters removal, convert numbers to spelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lower(col_name,dataF):\n",
    "    \"\"\"\n",
    "        Convert all words to lowercase.\n",
    "        \n",
    "        @P: \n",
    "        col (string): Name of column in a dataframe that contains text data\n",
    "        dataF (dataframe): Dataframe with the text data that needs modifying\n",
    "        \n",
    "    \"\"\"\n",
    "    dataF[col_name]=dataF[col_name].str.lower()\n",
    "    \n",
    "def remove_punctuation(col_name,dataF):\n",
    "    \"\"\"\n",
    "        Remove the punctuation in a column with text data.\n",
    "        \n",
    "        @P: \n",
    "        col (string): Name of column in a dataframe that contains text data\n",
    "        dataF (dataframe): Dataframe with the text data that needs modifying\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    #Regex to identify anything that is not a word or string\n",
    "    dataF[col_name]=dataF[col_name].str.replace(r'[^\\w\\s]+','') \n",
    "\n",
    "def remove_tag(txt_col):\n",
    "    \"\"\"\n",
    "        Remove the @'username' in a column with text data.\n",
    "        \n",
    "        @P: \n",
    "        txt_col (series): Dataframe column that contains text data.\n",
    "        \n",
    "        @R:\n",
    "        s (string): String without @'username'.\n",
    "        \n",
    "    \"\"\"\n",
    "    words = word_tokenize(txt_col)\n",
    "    \n",
    "    j=-1\n",
    "    w_arr=[]\n",
    "    for i in range(len(words)):\n",
    "        if words[i]=='@': j=i+1\n",
    "        \n",
    "        if words[i]!='@' and i!=j: w_arr.append(words[i])\n",
    "    \n",
    "    s=\" \".join(w_arr)\n",
    "    return s\n",
    "\n",
    "def remove_url(txt_col):\n",
    "    \"\"\"\n",
    "        Remove the url in a column with text data.\n",
    "        \n",
    "        @P: \n",
    "        txt_col (series): Dataframe column that contains text data.\n",
    "        \n",
    "        @R:\n",
    "        s (string): String without url.\n",
    "        \n",
    "    \"\"\"\n",
    "    words = word_tokenize(txt_col)\n",
    "    w_arr=[ w for w in words if not w.startswith('http')]\n",
    "    s=\" \".join(w_arr)\n",
    "    return s\n",
    "\n",
    "def lematize_txt(txt_col):\n",
    "    \"\"\"\n",
    "        Stem words.\n",
    "        \n",
    "        @P: \n",
    "        txt_col (series): Dataframe column that contains text data.\n",
    "        \n",
    "        @R:\n",
    "        s (string): String of lematized text\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(txt_col)\n",
    "    w_arr=[ps.stem(w) for w in words]\n",
    "    s=\" \".join(w_arr)\n",
    "    return s\n",
    "\n",
    "def rmv_stop_wrds(txt_col):\n",
    "    \"\"\"\n",
    "        Remove stop words.\n",
    "        \n",
    "        @P: \n",
    "        txt_col (series): Dataframe column that contains text data.\n",
    "        \n",
    "        @R:\n",
    "        s (string): String of text without stop words.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(txt_col)\n",
    "    w_arr=[w for w in words if not w.lower() in stop_words]\n",
    "    s=\" \".join(w_arr)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"input/train.csv\")\n",
    "\n",
    "make_lower(\"text\",df)\n",
    "df['text'] = df.apply(lambda x: remove_tag(x['text']),axis=1)\n",
    "remove_punctuation(\"text\",df)\n",
    "df['text'] = df.apply(lambda x: rmv_stop_wrds(x['text']),axis=1)\n",
    "df['text'] = df.apply(lambda x: remove_url(x['text']),axis=1)\n",
    "\n",
    "df.to_csv(\"output/ck_pre_proc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('target', axis=1, inplace=False)\n",
    "y=df['target']\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(df, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15775\n"
     ]
    }
   ],
   "source": [
    "#Determine the threshold using the 80-20 rule. \n",
    "#Plot the relative distribution of the words in the dataset\n",
    "\n",
    "#PLAY WITH THE THRESHOLD\n",
    "\n",
    "def all_docs(df,txt_col):\n",
    "    \"\"\"\n",
    "        Convert txt_col to list.\n",
    "        \n",
    "        @P:\n",
    "        col (series): the column in a dataframe containing the text data\n",
    "        voc_dict (dict): a dictionary where the keys as words and the frequency as the element\n",
    "        \n",
    "        @R:\n",
    "        voc_dict (dict): updated dictionary with the keys as words and the frequency as the element\n",
    "    \"\"\"\n",
    "    sent_arr=df[txt_col].to_list()\n",
    "    all_tok_lst=[word_tokenize(s) for s in sent_arr]\n",
    "    \n",
    "    distinct_tok=[list(set(a)) for a in all_tok_lst]\n",
    "    \n",
    "    all_toks=[]\n",
    "    for s in distinct_tok: all_toks.extend(s)\n",
    "    \n",
    "    voc_dict={}\n",
    "    for w in all_toks:\n",
    "        if w not in voc_dict: voc_dict[w]=1\n",
    "        else: voc_dict[w]+=1\n",
    "    \n",
    "    return voc_dict, len(sent_arr)\n",
    "\n",
    "def freq_lst(v_dict,total):\n",
    "    \"\"\"\n",
    "        Relative Frequency of the words in a list for sorting and percentiling\n",
    "        \n",
    "        @P:\n",
    "        v_dict (dict): a dictionary with relative frequency\n",
    "        \n",
    "        @R:\n",
    "        voc_list (list): a list with tuples of (words,relative_frequency)\n",
    "    \"\"\"\n",
    "    \n",
    "    voc_list=[(k,v_dict[k]/total) for k in v_dict]\n",
    "    return voc_list\n",
    "\n",
    "def sort_lst(freq_lst):\n",
    "    \"\"\"\n",
    "        Sort word frequency list by the frequency of the words in the list.\n",
    "        \n",
    "        @P:\n",
    "        freq_lst (list): list of tuples - (word,frequency)\n",
    "        \n",
    "        @R:\n",
    "        w_list (list): a list of tuples-(word,frequency) that are sorted by frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    return sorted(freq_lst, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "def pareto80(srt_freq,total):\n",
    "    \"\"\"\n",
    "        Find a desired percentile in the dataset.\n",
    "        \n",
    "        @p:\n",
    "        srt_freq (list): sorted list of words with relative frequency\n",
    "        \n",
    "        @r:\n",
    "        relative frequency at of the word at the 80% mark (pareto)\n",
    "        \n",
    "    \"\"\"\n",
    "    t=0.0\n",
    "    for w,f in srt_freq:\n",
    "        t+=f\n",
    "        if t>=0.8:return round(f*total)\n",
    "    \n",
    "\n",
    "def target_thresh(thresh,v):\n",
    "    \"\"\"\n",
    "        Create a vocab dictionary that has words that show up in 'thresh' number of tweets\n",
    "        \n",
    "        @p:\n",
    "        thresh (int): desired threshold\n",
    "        v (dict): all tokens in the dataset\n",
    "        \n",
    "        @r:\n",
    "        vocab (dict): a dictionary with words that have the desired frequency\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    print(len(v))\n",
    "    \n",
    "    vocab={}\n",
    "    for w in v:\n",
    "        if v[w]>thresh: vocab[w]=v[w]\n",
    "            \n",
    "    return vocab\n",
    "\n",
    "def bow(M,df,txt_col):\n",
    "    \"\"\"\n",
    "        Fit count vectorizer to the training dataset.\n",
    "        \n",
    "        @p:\n",
    "        M (int): desired threshold\n",
    "        df (datframe): training data\n",
    "        txt_col (string): name of the column in the dataframe with text data\n",
    "        \n",
    "        @r:\n",
    "        vectorizer (vectorizer): trained vectorizer\n",
    "        \n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(binary=True,min_df=M)\n",
    "    document=df[txt_col].to_list()\n",
    "    vectorizer.fit(document)\n",
    "    \n",
    "    return vectorizer\n",
    "            \n",
    "def encoder(vect,df,txt_col):\n",
    "    \"\"\"\n",
    "        Encode text data with encoder.\n",
    "        \n",
    "        @p:\n",
    "        vect (vectorizer): trained vectorizer\n",
    "        df (datframe): dataframe containing the information needed to be encoded\n",
    "        txt_col (string): name of the column in the dataframe with text data\n",
    "        \n",
    "        @r:\n",
    "        vector (scipy.sparse.csr.csr_matrix): encoded text data\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    document=df[txt_col].to_list()\n",
    "    vector = vect.transform(document)\n",
    "    \n",
    "    return vector\n",
    "    \n",
    "vocab_dict,total_tweets=all_docs(X_train,'text')\n",
    "vocab_freq_list=freq_lst(vocab_dict,total_tweets)\n",
    "v_freq_lst_sort=sort_lst(vocab_freq_list)\n",
    "\n",
    "threshold=pareto80(v_freq_lst_sort,total_tweets)\n",
    "\n",
    "final_v_dict=target_thresh(threshold,vocab_dict)\n",
    "\n",
    "train_vectorizer=bow(threshold,X_train,'text')\n",
    "\n",
    "v_text=encoder(train_vectorizer,X_train,'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = LogisticRegression(penalty='none').fit(v_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
