{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict whether or not a tweet is about a real disaster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "    \n",
    "    https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7219856"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview\n",
    "\n",
    "There are 7,613 training data points and 3,263 test datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_shape(filename):\n",
    "    \"\"\"\n",
    "        Return the shape (row,columns) of the dataset in the .csv file\n",
    "        \n",
    "        @P: filename (string) of a csv data set file\n",
    "        @R: tuple of dataframe size\n",
    "        \n",
    "    \"\"\"\n",
    "    df=pd.read_csv(filename)\n",
    "    return df.shape\n",
    "\n",
    "def hist_rel_freq(filename,columnName,mx_val):\n",
    "    \"\"\"\n",
    "        Save a relative frequency of a specific column with values that are numerical\n",
    "        \n",
    "        @P: filename: (csv) the training data file\n",
    "            columnName: (dtring) the name of the column of interest\n",
    "            mx_val: (int) the max range for the x-axis\n",
    "            \n",
    "    \"\"\"\n",
    "    df=pd.read_csv(filename)\n",
    "    fig, ax = plt.subplots()\n",
    "    g=sns.histplot(data=df, x=columnName, stat=\"percent\", discrete=True, ax=ax)\n",
    "    ax.set_xlim(-1,mx_val+1)\n",
    "    ax.set_xticks(range(0,mx_val+1))\n",
    "    plt.savefig('output/dist_{}.png'.format(columnName))\n",
    "    print(\"\\n Column: {} Relative Frequency \\n\".format(columnName),df[columnName].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training data is (7613, 5) \n",
      "The shape of the test data is (3263, 4) \n",
      "\n",
      " Column: target Relative Frequency \n",
      " 0    0.57034\n",
      "1    0.42966\n",
      "Name: target, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAON0lEQVR4nO3df6zddX3H8eeLAgIDBcZdg7RYBg0L+0GNd8iPZQk4F9xwdNGhzrn+0a0af2FcdGxx2ZaYRRejWxY3bYTRP1SKTFPsMjfSoWQiyC2ggEhABtIK9KL81CgU3vvjfO+4lNvew6Xfc3r7eT6Sm3O+33O+Pe+m6bPffs8532+qCklSOw4Y9wCSpNEy/JLUGMMvSY0x/JLUGMMvSY05cNwDDOOYY46pFStWjHsMSVpUtm7d+lBVTey6flGEf8WKFUxNTY17DElaVJLcO9d6D/VIUmMMvyQ1xvBLUmMMvyQ1xvBLUmMMvyQ1xvBLUmMMvyQ1xvBLUmMWxTd393fHLT+eH2y7b9xjaAFevmw52+/7/rjHkF4Qw78P+MG2+3jTp68d9xhagI1vP3PcI0gvmId6JKkxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGtPrxdaT3AM8DjwN7KyqySRHAxuBFcA9wAVV9XCfc0iSnjWKPf6zq2pVVU12yxcBW6pqJbClW5Ykjcg4DvWcD2zo7m8AVo9hBklqVt/hL+C/kmxNsq5bt7Sq7u/uPwAsnWvDJOuSTCWZmp6e7nlMSWpHr8f4gd+oqu1JfgG4Ksl3Zz9YVZWk5tqwqtYD6wEmJyfnfI4k6YXrdY+/qrZ3tzuALwGnAQ8mORagu93R5wySpOfqLfxJfi7JETP3gd8GbgWuBNZ0T1sDbOprBknS8/V5qGcp8KUkM6/zuar6SpIbgMuTrAXuBS7ocQZJ0i56C39V3Q2cOsf6HwKv6et1JUl75jd3Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGtPbxdalJhxwIEnGPYUW6OXLlrP9vu+Pe4yRM/zSi/HMTt706WvHPYUWaOPbzxz3CGPhoR5Jaozhl6TGGH5Jaozhl6TGGH5Jakzv4U+yJMlNSTZ3yyckuT7JXUk2Jjm47xkkSc8axR7/hcDts5Y/Cnyiqk4CHgbWjmAGSVKn1/AnWQb8LvCZbjnAOcAV3VM2AKv7nEGS9Fx97/H/A/BB4Jlu+eeBR6pqZ7e8DTiu5xkkSbP0Fv4k5wE7qmrrArdfl2QqydT09PRenk6S2tXnHv9ZwO8luQe4jMEhnn8Ejkwyc6qIZcD2uTauqvVVNVlVkxMTEz2OKUlt6S38VfUXVbWsqlYAbwb+u6reClwNvLF72hpgU18zSJKebxyf4/9z4P1J7mJwzP/iMcwgSc0aydk5q+qrwFe7+3cDp43idSVJz+c3dyWpMYZfkhpj+CWpMYZfkhpj+CWpMYZfkhpj+CWpMYZfkhpj+CWpMYZfkhpj+CWpMYZfkhpj+CWpMYZfkhpj+CWpMUOFP8lZw6yTJO37ht3j/6ch10mS9nF7vAJXkjOAM4GJJO+f9dBLgSV9DiZJ6sd8l148GDi8e94Rs9Y/xrMXTJckLSJ7DH9VfQ34WpJLq+reEc0kSerRsBdbf0mS9cCK2dtU1Tl9DCVJ6s+w4f8C8CngM8DT/Y0jSerbsOHfWVX/0uskkqSRGPbjnF9O8s4kxyY5euan18kkSb0Ydo9/TXf7gVnrCvjFvTuOJKlvQ4W/qk7oexBJ0mgMe8qGw5J8qPtkD0lWJjmv39EkSX0Y9hj/vwJPMvgWL8B24MO9TCRJ6tWw4T+xqv4eeAqgqn4CZE8bJDkkyTeTfCvJbUn+tlt/QpLrk9yVZGOSg1/U70CS9IIMG/4nkxzK4A1dkpwI/GyebX4GnFNVpwKrgHOTnA58FPhEVZ0EPAysXcjgkqSFGTb8fw18BVie5LPAFuCDe9qgBp7oFg/qfgo4B7iiW78BWP0CZ5YkvQjDfqrnqiQ3AqczOMRzYVU9NN92SZYAW4GTgE8C3wMeqaqd3VO2AcftZtt1wDqA448/fpgxJUlDGPZTPb/P4Nu7/15Vm4GdSVbPt11VPV1Vq4BlwGnALw07WFWtr6rJqpqcmJgYdjNJ0jyGPtRTVY/OLFTVIwwO/wyle/7VwBnAkUlm/qexjMEnhCRJIzJs+Od63nwXcZlIcmR3/1DgtcDtDP4BmDmX/xpg05AzSJL2gmFP2TCV5OMMjtMDvIvBsfs9ORbY0B3nPwC4vKo2J/kOcFmSDwM3ARcvYG5J0gING/73AH8FbGTwyZyrGMR/t6rq28Ar51h/N4Pj/ZKkMZg3/N0e++aqOnsE80iSejbvMf6qehp4JsnLRjCPJKlnwx7qeQK4JclVwI9nVlbVe3uZSpLUm2HD/8XuR5K0yA37zd0N3Ucyj6+qO3qeSZLUo2G/uft64GYG5+shyaokV/Y4lySpJ8N+getvGHwE8xGAqroZL7soSYvSsOF/avYpGzrP7O1hJEn9G/bN3duS/CGwJMlK4L3Atf2NJUnqy7B7/O8BfpnBxVU+BzwKvK+nmSRJPZrvRGuHAO9gcD79W4AzZp1LX5K0CM23x78BmGQQ/dcBH+t9IklSr+Y7xn9KVf0qQJKLgW/2P5IkqU/z7fE/NXPHQzyStH+Yb4//1CSPdfcDHNoth8H11F/a63SSpL1uj+GvqiWjGkSSNBrDfpxTkrSfMPyS1BjDL0mNMfyS1BjDL0mNMfyS1BjDL0mNMfyS1BjDL0mNMfyS1BjDL0mN6S38SZYnuTrJd5LcluTCbv3RSa5Kcmd3e1RfM0iSnq/PPf6dwJ9V1SnA6cC7kpwCXARsqaqVwJZuWZI0Ir2Fv6rur6obu/uPA7cDxwHnM7iyF93t6r5mkCQ930iO8SdZAbwSuB5YWlX3dw89ACzdzTbrkkwlmZqenh7FmJLUhN7Dn+Rw4N+A91XVY7Mfq6oCaq7tqmp9VU1W1eTExETfY0pSM3oNf5KDGET/s1X1xW71g0mO7R4/FtjR5wySpOfq81M9AS4Gbq+qj8966EpgTXd/DbCprxkkSc833zV3X4yzgLcBtyS5uVv3l8BHgMuTrAXuBS7ocQZJ0i56C39V/Q+Di7LP5TV9va4kac/85q4kNcbwS1JjDL8kNcbwS1JjDL8kNcbwS1JjDL8kNcbwS1JjDL8kNcbwS1JjDL8kNcbwS1JjDL8kNcbwS1JjDL8kNcbwS1JjDL8kNcbwS1JjDL8kNcbwS1JjDL8kNcbwS1JjDL8kNcbwS1JjDL8kNcbwS1JjDL8kNaa38Ce5JMmOJLfOWnd0kquS3NndHtXX60uS5tbnHv+lwLm7rLsI2FJVK4Et3bIkaYR6C39VXQP8aJfV5wMbuvsbgNV9vb4kaW6jPsa/tKru7+4/ACzd3ROTrEsylWRqenp6NNNJUgPG9uZuVRVQe3h8fVVNVtXkxMTECCeTpP3bqMP/YJJjAbrbHSN+fUlq3qjDfyWwpru/Btg04teXpOb1+XHOzwPfAE5Osi3JWuAjwGuT3An8VrcsSRqhA/v6havqLbt56DV9vaYkaX5+c1eSGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4Jakxhl+SGmP4JakxYwl/knOT3JHkriQXjWMGSWrVyMOfZAnwSeB1wCnAW5KcMuo5JKlV49jjPw24q6rurqongcuA88cwhyQ1KVU12hdM3gicW1V/0i2/DXh1Vb17l+etA9Z1iycDd4x0UO0txwAPjXsILZh/fovbK6pqYteVB45jkmFU1Xpg/bjn0IuTZKqqJsc9hxbGP7/90zgO9WwHls9aXtatkySNwDjCfwOwMskJSQ4G3gxcOYY5JKlJIz/UU1U7k7wb+E9gCXBJVd026jk0Mh6uW9z889sPjfzNXUnSePnNXUlqjOGXpMYYfvXGU3MsXkkuSbIjya3jnkV7n+FXLzw1x6J3KXDuuIdQPwy/+uKpORaxqroG+NG451A/DL/6chxw36zlbd06SWNm+CWpMYZfffHUHNI+yvCrL56aQ9pHGX71oqp2AjOn5rgduNxTcyweST4PfAM4Ocm2JGvHPZP2Hk/ZIEmNcY9fkhpj+CWpMYZfkhpj+CWpMYZfkhpj+NW8JEcmeecIXme1J6rTvsDwS3AkMHT4M7CQvzurGZypVBorP8ev5iWZOXPoHcDVwK8BRwEHAR+qqk1JVjD4Mtr1wKuA3wH+GPgjYJrBCem2VtXHkpzI4JTUE8BPgD8FjgY2A492P2+oqu+N6vcozTbyi61L+6CLgF+pqlVJDgQOq6rHkhwDXJdk5lQTK4E1VXVdkl8H3gCcyuAfiBuBrd3z1gPvqKo7k7wa+OeqOqf7dTZX1RWj/M1JuzL80nMF+Lskvwk8w+BU0ku7x+6tquu6+2cBm6rqp8BPk3wZIMnhwJnAF5LM/JovGdXw0jAMv/Rcb2VwiOZVVfVUknuAQ7rHfjzE9gcAj1TVqn7Gk14839yV4HHgiO7+y4AdXfTPBl6xm22+Drw+ySHdXv55AFX1GPC/Sf4A/v+N4FPneB1pbAy/mldVPwS+3l1YfBUwmeQWBm/efnc329zA4DTT3wb+A7iFwZu2MPhfw9ok3wJu49lLTl4GfCDJTd0bwNJY+KkeaYGSHF5VTyQ5DLgGWFdVN457Lmk+HuOXFm5994WsQ4ANRl+LhXv8ktQYj/FLUmMMvyQ1xvBLUmMMvyQ1xvBLUmP+Dzd2NzE1ZWkfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The shape of the training data is {} \".format(df_shape(\"input/train.csv\")))\n",
    "print(\"The shape of the test data is {} \".format(df_shape(\"input/test.csv\")))\n",
    "hist_rel_freq(\"input/train.csv\",\"target\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process\n",
    "\n",
    "• Convert all the words to lowercase: I converted the words to lowercase to standardize the tokens.\n",
    "\n",
    "• Lemmatize all thewords: I decided not to lemmative all the words because I wanted to maintain the variation between the words to preserve some of the writing style and variation.\n",
    "\n",
    "• Strip punctuation: Similar to making the words lowercase, I removed the punctuation to standardize the tokens. Also, given the nature of the text, punctuation do not really carry much sentiment/meaning.\n",
    "\n",
    "• Strip the stop words, e.g., “the”, “and”, “or”: I used a standard library to remove stop words to prevent any noise in the analysis. For future steps, I would want to adjust the definition of stop words to be more expansive.\n",
    "\n",
    "• Strip @ and urls (It’s Twitter.): I removed these for similar reasons to making the words lowercase and removing punctuation. It would be very frequent given the datasource, but not carry much meaning in the task.\n",
    "\n",
    "• Something else? Tell us about it: If I had more time, I would like to include a spell check specific to Twitter (Carnegie Mellon), repeated letters removal, and convert numbers to spelling to prevent those from being considered different from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lower(col_name,dataF):\n",
    "    \"\"\"\n",
    "        Convert all words to lowercase.\n",
    "        \n",
    "        @P: \n",
    "        col (string): Name of column in a dataframe that contains text data\n",
    "        dataF (dataframe): Dataframe with the text data that needs modifying\n",
    "        \n",
    "    \"\"\"\n",
    "    dataF[col_name]=dataF[col_name].str.lower()\n",
    "    \n",
    "def remove_punctuation(col_name,dataF):\n",
    "    \"\"\"\n",
    "        Remove the punctuation in a column with text data.\n",
    "        \n",
    "        @P: \n",
    "        col (string): Name of column in a dataframe that contains text data\n",
    "        dataF (dataframe): Dataframe with the text data that needs modifying\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    #Regex to identify anything that is not a word or string\n",
    "    dataF[col_name]=dataF[col_name].str.replace(r'[^\\w\\s]+','') \n",
    "\n",
    "def remove_tag(txt_col):\n",
    "    \"\"\"\n",
    "        Remove the @'username' in a column with text data.\n",
    "        \n",
    "        @P: \n",
    "        txt_col (series): Dataframe column that contains text data.\n",
    "        \n",
    "        @R:\n",
    "        s (string): String without @'username'.\n",
    "        \n",
    "    \"\"\"\n",
    "    words = word_tokenize(txt_col)\n",
    "    \n",
    "    j=-1\n",
    "    w_arr=[]\n",
    "    for i in range(len(words)):\n",
    "        if words[i]=='@': j=i+1\n",
    "        \n",
    "        if words[i]!='@' and i!=j: w_arr.append(words[i])\n",
    "    \n",
    "    s=\" \".join(w_arr)\n",
    "    return s\n",
    "\n",
    "def remove_url(txt_col):\n",
    "    \"\"\"\n",
    "        Remove the url in a column with text data.\n",
    "        \n",
    "        @P: \n",
    "        txt_col (series): Dataframe column that contains text data.\n",
    "        \n",
    "        @R:\n",
    "        s (string): String without url.\n",
    "        \n",
    "    \"\"\"\n",
    "    words = word_tokenize(txt_col)\n",
    "    w_arr=[ w for w in words if not w.startswith('http')]\n",
    "    s=\" \".join(w_arr)\n",
    "    return s\n",
    "\n",
    "def lematize_txt(txt_col):\n",
    "    \"\"\"\n",
    "        Stem words.\n",
    "        \n",
    "        @P: \n",
    "        txt_col (series): Dataframe column that contains text data.\n",
    "        \n",
    "        @R:\n",
    "        s (string): String of lematized text\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(txt_col)\n",
    "    w_arr=[ps.stem(w) for w in words]\n",
    "    s=\" \".join(w_arr)\n",
    "    return s\n",
    "\n",
    "def rmv_stop_wrds(txt_col):\n",
    "    \"\"\"\n",
    "        Remove stop words.\n",
    "        \n",
    "        @P: \n",
    "        txt_col (series): Dataframe column that contains text data.\n",
    "        \n",
    "        @R:\n",
    "        s (string): String of text without stop words.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(txt_col)\n",
    "    w_arr=[w for w in words if not w.lower() in stop_words]\n",
    "    s=\" \".join(w_arr)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"input/train.csv\")\n",
    "\n",
    "make_lower(\"text\",df)\n",
    "df['text'] = df.apply(lambda x: remove_tag(x['text']),axis=1)\n",
    "remove_punctuation(\"text\",df)\n",
    "df['text'] = df.apply(lambda x: rmv_stop_wrds(x['text']),axis=1)\n",
    "df['text'] = df.apply(lambda x: remove_url(x['text']),axis=1)\n",
    "\n",
    "df.to_csv(\"output/ck_pre_proc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('target', axis=1, inplace=False)\n",
    "y=df['target']\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(df, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model:\n",
    "\n",
    "I set the threshold M using the Pareto Principle. In industry, I learned about the 80-20 rule, which says that 80% of the results are driven by 20% of the cause. It's popular in business analysis and I wanted to see how it would work here. I set the threshold to be a word that shows up in atleast 69 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "39\n",
      "(5329, 5)\n",
      "(5329, 39)\n"
     ]
    }
   ],
   "source": [
    "#Determine the threshold using the 80-20 rule. \n",
    "#Plot the relative distribution of the words in the dataset\n",
    "\n",
    "#PLAY WITH THE THRESHOLD\n",
    "\n",
    "def all_docs(df,txt_col):\n",
    "    \"\"\"\n",
    "        Convert txt_col to list.\n",
    "        \n",
    "        @P:\n",
    "        col (series): the column in a dataframe containing the text data\n",
    "        voc_dict (dict): a dictionary where the keys as words and the frequency as the element\n",
    "        \n",
    "        @R:\n",
    "        voc_dict (dict): updated dictionary with the keys as words and the frequency as the element\n",
    "    \"\"\"\n",
    "    sent_arr=df[txt_col].to_list()\n",
    "    all_tok_lst=[word_tokenize(s) for s in sent_arr]\n",
    "    \n",
    "    distinct_tok=[list(set(a)) for a in all_tok_lst]\n",
    "    \n",
    "    all_toks=[]\n",
    "    for s in distinct_tok: all_toks.extend(s)\n",
    "    \n",
    "    voc_dict={}\n",
    "    for w in all_toks:\n",
    "        if w not in voc_dict: voc_dict[w]=1\n",
    "        else: voc_dict[w]+=1\n",
    "    \n",
    "    return voc_dict, len(sent_arr)\n",
    "\n",
    "def freq_lst(v_dict,total):\n",
    "    \"\"\"\n",
    "        Relative Frequency of the words in a list for sorting and percentiling\n",
    "        \n",
    "        @P:\n",
    "        v_dict (dict): a dictionary with relative frequency\n",
    "        \n",
    "        @R:\n",
    "        voc_list (list): a list with tuples of (words,relative_frequency)\n",
    "    \"\"\"\n",
    "    \n",
    "    voc_list=[(k,v_dict[k]/total) for k in v_dict]\n",
    "    return voc_list\n",
    "\n",
    "def sort_lst(freq_lst):\n",
    "    \"\"\"\n",
    "        Sort word frequency list by the frequency of the words in the list.\n",
    "        \n",
    "        @P:\n",
    "        freq_lst (list): list of tuples - (word,frequency)\n",
    "        \n",
    "        @R:\n",
    "        w_list (list): a list of tuples-(word,frequency) that are sorted by frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    return sorted(freq_lst, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "def pareto80(srt_freq,total):\n",
    "    \"\"\"\n",
    "        Find a desired percentile in the dataset.\n",
    "        \n",
    "        @p:\n",
    "        srt_freq (list): sorted list of words with relative frequency\n",
    "        \n",
    "        @r:\n",
    "        relative frequency at of the word at the 80% mark (pareto)\n",
    "        \n",
    "    \"\"\"\n",
    "    t=0.0\n",
    "    for w,f in srt_freq:\n",
    "        t+=f\n",
    "        if t>=0.8:return round(f*total)\n",
    "    \n",
    "\n",
    "def target_thresh(thresh,v):\n",
    "    \"\"\"\n",
    "        Create a vocab dictionary that has words that show up in 'thresh' number of tweets\n",
    "        \n",
    "        @p:\n",
    "        thresh (int): desired threshold\n",
    "        v (dict): all tokens in the dataset\n",
    "        \n",
    "        @r:\n",
    "        vocab (dict): a dictionary with words that have the desired frequency\n",
    "        \n",
    "    \"\"\"\n",
    "    vocab={}\n",
    "    for w in v:\n",
    "        if v[w]>thresh: vocab[w]=v[w]\n",
    "            \n",
    "    return vocab\n",
    "\n",
    "def bow(M,df,txt_col):\n",
    "    \"\"\"\n",
    "        Fit count vectorizer to the training dataset.\n",
    "        \n",
    "        @p:\n",
    "        M (int): desired threshold\n",
    "        df (datframe): training data\n",
    "        txt_col (string): name of the column in the dataframe with text data\n",
    "        \n",
    "        @r:\n",
    "        vectorizer (vectorizer): trained vectorizer\n",
    "        \n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(binary=True,min_df=M)\n",
    "    document=df[txt_col].to_list()\n",
    "    vectorizer.fit(document)\n",
    "    \n",
    "    return vectorizer\n",
    "            \n",
    "def encoder(vect,df,txt_col):\n",
    "    \"\"\"\n",
    "        Encode text data with encoder.\n",
    "        \n",
    "        @p:\n",
    "        vect (vectorizer): trained vectorizer\n",
    "        df (datframe): dataframe containing the information needed to be encoded\n",
    "        txt_col (string): name of the column in the dataframe with text data\n",
    "        \n",
    "        @r:\n",
    "        vector (scipy.sparse.csr.csr_matrix): encoded text data\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    document=df[txt_col].to_list()\n",
    "    vector = vect.transform(document)\n",
    "    \n",
    "    return vector\n",
    "    \n",
    "vocab_dict,total_tweets=all_docs(X_train,'text')\n",
    "vocab_freq_list=freq_lst(vocab_dict,total_tweets)\n",
    "v_freq_lst_sort=sort_lst(vocab_freq_list)\n",
    "\n",
    "threshold=pareto80(v_freq_lst_sort,total_tweets)\n",
    "print(threshold)\n",
    "\n",
    "final_v_dict=target_thresh(threshold,vocab_dict)\n",
    "print(len(final_v_dict))\n",
    "\n",
    "train_vectorizer=bow(threshold,X_train,'text')\n",
    "print(X_train.shape)\n",
    "\n",
    "v_text=encoder(train_vectorizer,X_train,'text')\n",
    "print(v_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression without Regularization\n",
    "\n",
    "I checked the f1 score on both the training and development set and got similar performance of 0.45 for both cases. Since both of these values are about the same, I do not think that there is under or overfitting in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = LogisticRegression(penalty='none').fit(v_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4460388639760837"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred=lf.predict(v_text)\n",
    "f1_score(y_train, train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44805653710247345"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_text_val=encoder(train_vectorizer,X_validate,'text')\n",
    "val_pred=lf.predict(v_text_val)\n",
    "f1_score(y_validate, val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with L1 Regularization\n",
    "\n",
    "I checked the f1 score on both the training and development set and got similar performance of 0.45 for both cases. Since both of these values are about the same, I do not think that there is under or overfitting in the model. Interestingly, the L1 regularization model performs just as well as th one without the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf1 = LogisticRegression(penalty='l1',solver='liblinear').fit(v_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4445775913720791"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred=lf1.predict(v_text)\n",
    "f1_score(y_train, train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4497878359264498"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_text_val=encoder(train_vectorizer,X_validate,'text')\n",
    "val_pred=lf1.predict(v_text_val)\n",
    "f1_score(y_validate, val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with L2 Regularization\n",
    "\n",
    "I checked the f1 score on both the training and development set and got similar performance of 0.45 for both cases. Since both of these values are about the same, I do not think that there is under or overfitting in the model. Interestingly, the L1 regularization model performs just as well as th one without the regularization and the L2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf2 = LogisticRegression(penalty='l2').fit(v_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44704470447044703"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred=lf2.predict(v_text)\n",
    "f1_score(y_train, train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4499645138396026"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_text_val=encoder(train_vectorizer,X_validate,'text')\n",
    "val_pred=lf2.predict(v_text_val)\n",
    "f1_score(y_validate, val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All 3 models perform the same since all the f1 values are ~0.45 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 Reg Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0428866934207792"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_l1=lf1.coef_\n",
    "coef_l1_lst=coef_l1.tolist()\n",
    "coef_l1_lst[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['man', 3.2438866191214504],\n",
       " ['crash', 2.5114775826441065],\n",
       " ['2', 1.7451553879685358],\n",
       " ['burning', -1.5294684024579186],\n",
       " ['via', 1.4903367458559897],\n",
       " ['first', 1.4215184955314966],\n",
       " ['got', 1.3072015296544293],\n",
       " ['day', 1.2699716466178632],\n",
       " ['go', 1.0609350874202585],\n",
       " ['still', 1.0428866934207792],\n",
       " ['buildings', -1.0221281861503886],\n",
       " ['world', 0.9947383205853492],\n",
       " ['fires', 0.8558790645813771],\n",
       " ['news', -0.8420573861389427],\n",
       " ['3', -0.8245511044505313],\n",
       " ['california', 0.8228891603059516],\n",
       " ['like', -0.7232597442821738],\n",
       " ['get', -0.6939301322459069],\n",
       " ['disaster', 0.5305629831448346],\n",
       " ['video', -0.5185159547756655],\n",
       " ['know', -0.493086247040351],\n",
       " ['see', -0.4766373178400977],\n",
       " ['nt', -0.4380206662653177],\n",
       " ['fire', -0.4298083730757974],\n",
       " ['suicide', -0.42290688655261116],\n",
       " ['would', -0.4023111241773016],\n",
       " ['nuclear', 0.38441163747534535],\n",
       " ['us', 0.3801180927848428],\n",
       " ['time', -0.34728447708538857],\n",
       " ['back', 0.282314489997885],\n",
       " ['body', -0.22813312790270582],\n",
       " ['police', -0.22336158150299365],\n",
       " ['people', -0.2022758867705667],\n",
       " ['attack', 0.16062055571552455],\n",
       " ['life', 0.13146396159028553],\n",
       " ['new', -0.09438337431271561],\n",
       " ['one', 0.07683417871188782],\n",
       " ['amp', 0.012409702616103578],\n",
       " ['emergency', 0.0]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc=[[k,0] for k in final_v_dict]\n",
    "\n",
    "for i in range(len(coef_l1_lst[0])):\n",
    "    voc[i][1]=coef_l1_lst[0][i]\n",
    "\n",
    "sorted(voc, key=lambda tup: abs(tup[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important words for model performance are: 'man','crash', '2' and 'burning.' I think this shows that the model has potential, but by adjusting the threshold and maybe adjusting the features, I may be able to get better results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 0 0 0]\n",
      "2284\n"
     ]
    }
   ],
   "source": [
    "n = v_text.shape[0] # size of the dataset\n",
    "d = v_text.shape[1] # number of features in our dataset\n",
    "K = 2\n",
    "\n",
    "# these are the shapes of the parameters\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])\n",
    "\n",
    "# we now compute the parameters\n",
    "v_train=pd.DataFrame(v_text.todense())\n",
    "y_train_d = v_train.set_index(y_train.index)\n",
    "\n",
    "for k in range(K):\n",
    "    #X_k = v_train[y_train==k]\n",
    "    X_k = y_train_d[y_train==k]\n",
    "    psis[k] = np.mean(X_k, axis=0) #Probability a doc contains a word in the dictionary\n",
    "    phis[k] = X_k.shape[0] / float(n) #Probability of a class\n",
    "\n",
    "\n",
    "# we can implement this in numpy\n",
    "def nb_predictions(x, psis, phis):\n",
    "    \"\"\"\n",
    "        This returns class assignments and scores under the NB model.\n",
    "        We compute \\arg\\max_y p(y|x) as \\arg\\max_y p(x|y)p(y)\n",
    "    \"\"\"\n",
    "    # adjust shapes\n",
    "    n, d = x.shape\n",
    "    x = np.reshape(x, (1, n, d))\n",
    "    x = np.reshape(x, (1, n, d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    # clip probabilities to avoid log(0)\n",
    "    psis = psis.clip(1e-14, 1-1e-14)\n",
    "    # compute log-probabilities\n",
    "    logpy = np.log(phis).reshape([K,1])\n",
    "    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "    \n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])\n",
    "    \n",
    "idx, logpyx = nb_predictions(v_text_val.toarray(), psis, phis)\n",
    "print(idx[:10])\n",
    "print(len(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4573426573426573"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_validate, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "\n",
    "I trained 4 models: Logistic Regression, Logistic Regression L1, Logistic Regression L2, and Bernoulli Naive Bayes. All 3 Regression models have f1 performances of ~0.45, while Bernoulli Naive Bayes has a f1 performance of 0.46, which is slightly better than the Regression Models. \n",
    "\n",
    "A discriminative model, like the Bayes model, predicts on the validation/development data based on conditional probability. While a generative model, like Logistic Regression, focuses on the distribution of a dataset to return a probability for a given example. In this example, both worked very similarly, hinting that the features/pre-processing needs to be updated.\n",
    "\n",
    "Naive Bayes assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature in the class, which it a fair assumption for this classification task because we are more interested in the the prescence of a word, rather than the postion of a word in a document. However, in a given situation, it is nice to also take context into account because the words before and after the word changes the interpretation of the word. For example, BERT has attention which means that is accounts interprets the word, 'bank' differentently for 'river bank' and 'money bank.'\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/07/deep-understanding-of-discriminative-and-generative-models-in-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Model\n",
    "\n",
    "The results from using N-grams was significantly better than previous results. The mix of 1-grams and 2-grams with a threshold of 7 tweets (80-20 rule). Context is important in a task like this - as shown in the 10 bigrams. The bigrams include 'mass murder' and 'suicide bomber.'\n",
    "\n",
    "Logistic Regression (No Reg) Training: 0.9\n",
    "Logistic Regression (No Reg) Development: 0.7 (Signs of overfitting)\n",
    "Bayes: 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 bigrams: \n",
      " [(('ca', 'nt'), 0.011634452993056859), (('suicide', 'bomber'), 0.007130793769938075), (('burning', 'buildings'), 0.006380183899418277), (('northern', 'california'), 0.00562957402889848), (('gon', 'na'), 0.005254269093638581), (('liked', 'video'), 0.005254269093638581), (('california', 'wildfire'), 0.005066616626008632), (('cross', 'body'), 0.0046913116907487335), (('oil', 'spill'), 0.0046913116907487335), (('mass', 'murder'), 0.0046913116907487335)]\n",
      "7\n",
      "320\n",
      "(5329, 5)\n",
      "(5329, 1827)\n"
     ]
    }
   ],
   "source": [
    "def all_docs_ngrams(df,txt_col,n):\n",
    "    \"\"\"\n",
    "        Convert txt_col to list, and provide a dictionary of all n-grams with frequency.\n",
    "        \n",
    "        @P:\n",
    "        col (series): the column in a dataframe containing the text data\n",
    "        voc_dict (dict): a dictionary where the keys as words and the frequency as the element\n",
    "        \n",
    "        @R:\n",
    "        ngram_dict (dict): updated dictionary with the keys as words and the frequency as the element\n",
    "    \"\"\"\n",
    "    sent_arr=df[txt_col].to_list()\n",
    "    all_tok_lst=[ngrams(s.split(), n) for s in sent_arr]\n",
    "    \n",
    "    distinct_tok=[list(set(a)) for a in all_tok_lst]\n",
    "    \n",
    "    all_toks=[]\n",
    "    for s in distinct_tok: all_toks.extend(s)\n",
    "    \n",
    "    voc_dict={}\n",
    "    for w in all_toks:\n",
    "        if w not in voc_dict: voc_dict[w]=1\n",
    "        else: voc_dict[w]+=1\n",
    "    \n",
    "    return voc_dict, len(sent_arr)\n",
    "\n",
    "\n",
    "def bow_ngram(M,df,txt_col):\n",
    "    \"\"\"\n",
    "        Fit count vectorizer to the training dataset.\n",
    "        \n",
    "        @p:\n",
    "        M (int): desired threshold\n",
    "        df (datframe): training data\n",
    "        txt_col (string): name of the column in the dataframe with text data\n",
    "        \n",
    "        @r:\n",
    "        vectorizer (vectorizer): trained vectorizer\n",
    "        \n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(binary=True,min_df=M,ngram_range=(1, 2))\n",
    "    document=df[txt_col].to_list()\n",
    "    vectorizer.fit(document)\n",
    "    \n",
    "    return vectorizer\n",
    "\n",
    "_2gram_dict,total_tweets=all_docs_ngrams(X_train,'text',2)\n",
    "vocab_freq_list_ngrm=freq_lst_tok(_2gram_dict,total_tweets)\n",
    "v_freq_lst_sort_ngrm=sort_lst(vocab_freq_list_ngrm)\n",
    "\n",
    "print(\"10 bigrams: \\n\",v_freq_lst_sort_ngrm[:10])\n",
    "\n",
    "threshold=pareto80(v_freq_lst_sort_ngrm,total_tweets)\n",
    "print(threshold)\n",
    "train_vectorizer=bow_ngram(threshold,X_train,'text')\n",
    "\n",
    "final_v_dict_ngram=target_thresh(threshold,_2gram_dict)\n",
    "print(len(final_v_dict_ngram))\n",
    "\n",
    "train_vectorizer=bow_ngram(threshold,X_train,'text')\n",
    "print(X_train.shape)\n",
    "\n",
    "v_text=encoder(train_vectorizer,X_train,'text')\n",
    "print(v_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9064301552106431"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf = LogisticRegression(penalty='none').fit(v_text, y_train)\n",
    "train_pred=lf.predict(v_text)\n",
    "f1_score(y_train, train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7017017017017018"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_text_val=encoder(train_vectorizer,X_validate,'text')\n",
    "val_pred=lf.predict(v_text_val)\n",
    "f1_score(y_validate, val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "2284\n"
     ]
    }
   ],
   "source": [
    "n = v_text.shape[0] # size of the dataset\n",
    "d = v_text.shape[1] # number of features in our dataset\n",
    "K = 2\n",
    "\n",
    "# these are the shapes of the parameters\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])\n",
    "\n",
    "# we now compute the parameters\n",
    "v_train=pd.DataFrame(v_text.todense())\n",
    "y_train_d = v_train.set_index(y_train.index)\n",
    "\n",
    "for k in range(K):\n",
    "    #X_k = v_train[y_train==k]\n",
    "    X_k = y_train_d[y_train==k]\n",
    "    psis[k] = np.mean(X_k, axis=0) #Probability a doc contains a word in the dictionary\n",
    "    phis[k] = X_k.shape[0] / float(n) #Probability of a class\n",
    "    \n",
    "idx, logpyx = nb_predictions(v_text_val.toarray(), psis, phis)\n",
    "print(idx[:10])\n",
    "print(len(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6963414634146341"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_validate, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance with Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
