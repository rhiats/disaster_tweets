{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict whether or not a tweet is about a real disaster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "    \n",
    "    https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7219856"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/__check_build/_check_build.cpython-36m-darwin.so, 2): Symbol not found: ____chkstk_darwin\n  Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/__check_build/../.dylibs/libomp.dylib (which was built for Mac OS X 10.15)\n  Expected in: /usr/lib/libSystem.B.dylib\n in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/__check_build/../.dylibs/libomp.dylib\n___________________________________________________________________________\nContents of /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/__check_build:\n__init__.py               __pycache__               _check_build.cpython-36m-darwin.so\nsetup.py\n___________________________________________________________________________\nIt seems that scikit-learn has not been built correctly.\n\nIf you have installed scikit-learn from source, please do not forget\nto build the package before using it: run `python setup.py install` or\n`make` in the source directory.\n\nIf you have used an installer, please check that it is suited for your\nPython version, your operating system and your platform.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/__check_build/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_check_build\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_build\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/__check_build/_check_build.cpython-36m-darwin.so, 2): Symbol not found: ____chkstk_darwin\n  Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/__check_build/../.dylibs/libomp.dylib (which was built for Mac OS X 10.15)\n  Expected in: /usr/lib/libSystem.B.dylib\n in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/__check_build/../.dylibs/libomp.dylib",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-75c2e3793df9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/__check_build/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_check_build\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_build\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mraise_build_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/__check_build/__init__.py\u001b[0m in \u001b[0;36mraise_build_error\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbuild\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpackage\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0musing\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpython\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[0minstall\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m`\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msource\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m %s\"\"\" % (e, local_dir, ''.join(dir_content).strip(), msg))\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/__check_build/_check_build.cpython-36m-darwin.so, 2): Symbol not found: ____chkstk_darwin\n  Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/__check_build/../.dylibs/libomp.dylib (which was built for Mac OS X 10.15)\n  Expected in: /usr/lib/libSystem.B.dylib\n in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/__check_build/../.dylibs/libomp.dylib\n___________________________________________________________________________\nContents of /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/__check_build:\n__init__.py               __pycache__               _check_build.cpython-36m-darwin.so\nsetup.py\n___________________________________________________________________________\nIt seems that scikit-learn has not been built correctly.\n\nIf you have installed scikit-learn from source, please do not forget\nto build the package before using it: run `python setup.py install` or\n`make` in the source directory.\n\nIf you have used an installer, please check that it is suited for your\nPython version, your operating system and your platform."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview\n",
    "\n",
    "There are 7,613 training data points and 3,263 test datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_shape(filename):\n",
    "    \"\"\"\n",
    "        Return the shape (row,columns) of the dataset in the .csv file\n",
    "        \n",
    "        @P: filename (string) of a csv data set file\n",
    "        @R: tuple of dataframe size\n",
    "        \n",
    "    \"\"\"\n",
    "    df=pd.read_csv(filename)\n",
    "    return df.shape\n",
    "\n",
    "def hist_rel_freq(filename,columnName,mx_val):\n",
    "    \"\"\"\n",
    "        Save a relative frequency of a specific column with values that are numerical\n",
    "        \n",
    "        @P: filename: (csv) the training data file\n",
    "            columnName: (dtring) the name of the column of interest\n",
    "            mx_val: (int) the max range for the x-axis\n",
    "            \n",
    "    \"\"\"\n",
    "    df=pd.read_csv(filename)\n",
    "    fig, ax = plt.subplots()\n",
    "    g=sns.histplot(data=df, x=columnName, stat=\"percent\", discrete=True, ax=ax)\n",
    "    ax.set_xlim(-1,mx_val+1)\n",
    "    ax.set_xticks(range(0,mx_val+1))\n",
    "    plt.savefig('output/dist_{}.png'.format(columnName))\n",
    "    print(\"\\n Column: {} Relative Frequency \\n\".format(columnName),df[columnName].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of the training data is {} \".format(df_shape(\"input/train.csv\")))\n",
    "print(\"The shape of the test data is {} \".format(df_shape(\"input/test.csv\")))\n",
    "hist_rel_freq(\"input/train.csv\",\"target\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process\n",
    "\n",
    "• Convert all the words to lowercase\n",
    "\n",
    "• Lemmatize all thewords (i.e., convert every word to its root so that all of “running,” “run,” and “runs” are converted to “run” and and all of “good,” “well,” “better,” and “best” are converted to “good”; this is easily done using nltk.stem) - (CHOSE NOT TO LEMMATIZE, TO PRESERVE MEANING OF TEXT - CONSIDER WRITING MY OWN FUNCTION)\n",
    "\n",
    "• Strip punctuation\n",
    "\n",
    "• Strip the stop words, e.g., “the”, “and”, “or” (ADDITIONAL STOP WORDS?)\n",
    "\n",
    "• Strip @ and urls (It’s Twitter.)\n",
    "\n",
    "• Something else? Tell us about it (SPELL CHECK, Repeated letters removal, convert numbers to spelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lower(col_name,dataF):\n",
    "    \"\"\"\n",
    "        Convert all words to lowercase.\n",
    "        \n",
    "        @P: \n",
    "        col (string): Name of column in a dataframe that contains text data\n",
    "        dataF (dataframe): Dataframe with the text data that needs modifying\n",
    "        \n",
    "    \"\"\"\n",
    "    dataF[col_name]=dataF[col_name].str.lower()\n",
    "    \n",
    "def remove_punctuation(col_name,dataF):\n",
    "    \"\"\"\n",
    "        Remove the punctuation in a column with text data.\n",
    "        \n",
    "        @P: \n",
    "        col (string): Name of column in a dataframe that contains text data\n",
    "        dataF (dataframe): Dataframe with the text data that needs modifying\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    #Regex to identify anything that is not a word or string\n",
    "    dataF[col_name]=dataF[col_name].str.replace(r'[^\\w\\s]+','') \n",
    "\n",
    "def remove_tag(txt_col):\n",
    "    \"\"\"\n",
    "        Remove the @'username' in a column with text data.\n",
    "        \n",
    "        @P: \n",
    "        txt_col (series): Dataframe column that contains text data.\n",
    "        \n",
    "        @R:\n",
    "        s (string): String without @'username'.\n",
    "        \n",
    "    \"\"\"\n",
    "    words = word_tokenize(txt_col)\n",
    "    \n",
    "    j=-1\n",
    "    w_arr=[]\n",
    "    for i in range(len(words)):\n",
    "        if words[i]=='@': j=i+1\n",
    "        \n",
    "        if words[i]!='@' and i!=j: w_arr.append(words[i])\n",
    "    \n",
    "    s=\" \".join(w_arr)\n",
    "    return s\n",
    "\n",
    "def remove_url(txt_col):\n",
    "    \"\"\"\n",
    "        Remove the url in a column with text data.\n",
    "        \n",
    "        @P: \n",
    "        txt_col (series): Dataframe column that contains text data.\n",
    "        \n",
    "        @R:\n",
    "        s (string): String without url.\n",
    "        \n",
    "    \"\"\"\n",
    "    words = word_tokenize(txt_col)\n",
    "    w_arr=[ w for w in words if not w.startswith('http')]\n",
    "    s=\" \".join(w_arr)\n",
    "    return s\n",
    "\n",
    "def lematize_txt(txt_col):\n",
    "    \"\"\"\n",
    "        Stem words.\n",
    "        \n",
    "        @P: \n",
    "        txt_col (series): Dataframe column that contains text data.\n",
    "        \n",
    "        @R:\n",
    "        s (string): String of lematized text\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(txt_col)\n",
    "    w_arr=[ps.stem(w) for w in words]\n",
    "    s=\" \".join(w_arr)\n",
    "    return s\n",
    "\n",
    "def rmv_stop_wrds(txt_col):\n",
    "    \"\"\"\n",
    "        Remove stop words.\n",
    "        \n",
    "        @P: \n",
    "        txt_col (series): Dataframe column that contains text data.\n",
    "        \n",
    "        @R:\n",
    "        s (string): String of text without stop words.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(txt_col)\n",
    "    w_arr=[w for w in words if not w.lower() in stop_words]\n",
    "    s=\" \".join(w_arr)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"input/train.csv\")\n",
    "\n",
    "make_lower(\"text\",df)\n",
    "df['text'] = df.apply(lambda x: remove_tag(x['text']),axis=1)\n",
    "remove_punctuation(\"text\",df)\n",
    "df['text'] = df.apply(lambda x: rmv_stop_wrds(x['text']),axis=1)\n",
    "df['text'] = df.apply(lambda x: remove_url(x['text']),axis=1)\n",
    "\n",
    "df.to_csv(\"output/ck_pre_proc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('target', axis=1, inplace=False)\n",
    "y=df['target']\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(df, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the threshold using the 80-20 rule. \n",
    "#Plot the relative distribution of the words in the dataset\n",
    "\n",
    "#PLAY WITH THE THRESHOLD\n",
    "\n",
    "def all_docs(df,txt_col):\n",
    "    \"\"\"\n",
    "        Convert txt_col to list.\n",
    "        \n",
    "        @P:\n",
    "        col (series): the column in a dataframe containing the text data\n",
    "        voc_dict (dict): a dictionary where the keys as words and the frequency as the element\n",
    "        \n",
    "        @R:\n",
    "        voc_dict (dict): updated dictionary with the keys as words and the frequency as the element\n",
    "    \"\"\"\n",
    "    sent_arr=df[txt_col].to_list()\n",
    "    all_tok_lst=[word_tokenize(s) for s in sent_arr]\n",
    "    \n",
    "    distinct_tok=[list(set(a)) for a in all_tok_lst]\n",
    "    \n",
    "    all_toks=[]\n",
    "    for s in distinct_tok: all_toks.extend(s)\n",
    "    \n",
    "    voc_dict={}\n",
    "    for w in all_toks:\n",
    "        if w not in voc_dict: voc_dict[w]=1\n",
    "        else: voc_dict[w]+=1\n",
    "    \n",
    "    return voc_dict, len(sent_arr)\n",
    "\n",
    "def freq_lst(v_dict,total):\n",
    "    \"\"\"\n",
    "        Relative Frequency of the words in a list for sorting and percentiling\n",
    "        \n",
    "        @P:\n",
    "        v_dict (dict): a dictionary with relative frequency\n",
    "        \n",
    "        @R:\n",
    "        voc_list (list): a list with tuples of (words,relative_frequency)\n",
    "    \"\"\"\n",
    "    \n",
    "    voc_list=[(k,v_dict[k]/total) for k in v_dict]\n",
    "    return voc_list\n",
    "\n",
    "def sort_lst(freq_lst):\n",
    "    \"\"\"\n",
    "        Sort word frequency list by the frequency of the words in the list.\n",
    "        \n",
    "        @P:\n",
    "        freq_lst (list): list of tuples - (word,frequency)\n",
    "        \n",
    "        @R:\n",
    "        w_list (list): a list of tuples-(word,frequency) that are sorted by frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    return sorted(freq_lst, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "def pareto80(srt_freq,total):\n",
    "    \"\"\"\n",
    "        Find a desired percentile in the dataset.\n",
    "        \n",
    "        @p:\n",
    "        srt_freq (list): sorted list of words with relative frequency\n",
    "        \n",
    "        @r:\n",
    "        relative frequency at of the word at the 80% mark (pareto)\n",
    "        \n",
    "    \"\"\"\n",
    "    t=0.0\n",
    "    for w,f in srt_freq:\n",
    "        t+=f\n",
    "        if t>=0.8:return round(f*total)\n",
    "    \n",
    "\n",
    "def target_thresh(thresh,v):\n",
    "    \"\"\"\n",
    "        Create a vocab dictionary that has words that show up in 'thresh' number of tweets\n",
    "        \n",
    "        @p:\n",
    "        thresh (int): desired threshold\n",
    "        v (dict): all tokens in the dataset\n",
    "        \n",
    "        @r:\n",
    "        vocab (dict): a dictionary with words that have the desired frequency\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    print(len(v))\n",
    "    \n",
    "    vocab={}\n",
    "    for w in v:\n",
    "        if v[w]>thresh: vocab[w]=v[w]\n",
    "            \n",
    "    return vocab\n",
    "\n",
    "def bow(M,df,txt_col):\n",
    "    \"\"\"\n",
    "        Fit count vectorizer to the training dataset.\n",
    "        \n",
    "        @p:\n",
    "        M (int): desired threshold\n",
    "        df (datframe): training data\n",
    "        txt_col (string): name of the column in the dataframe with text data\n",
    "        \n",
    "        @r:\n",
    "        vectorizer (vectorizer): trained vectorizer\n",
    "        \n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(binary=True,min_df=M)\n",
    "    document=df[txt_col].to_list()\n",
    "    vectorizer.fit(document)\n",
    "    \n",
    "    return vectorizer\n",
    "            \n",
    "def encoder(vect,df,txt_col):\n",
    "    \"\"\"\n",
    "        Encode text data with encoder.\n",
    "        \n",
    "        @p:\n",
    "        vect (vectorizer): trained vectorizer\n",
    "        df (datframe): dataframe containing the information needed to be encoded\n",
    "        txt_col (string): name of the column in the dataframe with text data\n",
    "        \n",
    "        @r:\n",
    "        vector (scipy.sparse.csr.csr_matrix): encoded text data\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    document=df[txt_col].to_list()\n",
    "    vector = vect.transform(document)\n",
    "    \n",
    "    return vector\n",
    "    \n",
    "vocab_dict,total_tweets=all_docs(X_train,'text')\n",
    "vocab_freq_list=freq_lst(vocab_dict,total_tweets)\n",
    "v_freq_lst_sort=sort_lst(vocab_freq_list)\n",
    "\n",
    "threshold=pareto80(v_freq_lst_sort,total_tweets)\n",
    "\n",
    "final_v_dict=target_thresh(threshold,vocab_dict)\n",
    "print(len(final_v_dict))\n",
    "\n",
    "train_vectorizer=bow(threshold,X_train,'text')\n",
    "print(X_train.shape)\n",
    "\n",
    "v_text=encoder(train_vectorizer,X_train,'text')\n",
    "print(v_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression without Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = LogisticRegression(penalty='none').fit(v_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred=lf.predict(v_text)\n",
    "f1_score(y_train, train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_text_val=encoder(train_vectorizer,X_validate,'text')\n",
    "val_pred=lf.predict(v_text_val)\n",
    "f1_score(y_validate, val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf1 = LogisticRegression(penalty='l1',solver='liblinear').fit(v_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred=lf1.predict(v_text)\n",
    "f1_score(y_train, train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_text_val=encoder(train_vectorizer,X_validate,'text')\n",
    "val_pred=lf1.predict(v_text_val)\n",
    "f1_score(y_validate, val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf2 = LogisticRegression(penalty='l2').fit(v_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred=lf2.predict(v_text)\n",
    "f1_score(y_train, train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v_text_val=encoder(train_vectorizer,X_validate,'text')\n",
    "val_pred=lf2.predict(v_text_val)\n",
    "f1_score(y_validate, val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 Reg Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_l1=lf1.coef_\n",
    "coef_l1_lst=coef_l1.tolist()\n",
    "coef_l1_lst[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc=[[k,0] for k in final_v_dict]\n",
    "\n",
    "for i in range(len(coef_l1_lst[0])):\n",
    "    voc[i][1]=coef_l1_lst[0][i]\n",
    "\n",
    "sorted(voc, key=lambda tup: abs(tup[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = v_text.shape[0] # size of the dataset\n",
    "d = len(final_v_dict) # number of features in our dataset\n",
    "K = 2\n",
    "\n",
    "# these are the shapes of the parameters\n",
    "psis = np.zeros([K,d])\n",
    "phis = np.zeros([K])\n",
    "\n",
    "# we now compute the parameters\n",
    "for k in range(K):\n",
    "    X_k = X_train[y_train == k]\n",
    "    psis[k] = np.mean(X_k, axis=0)\n",
    "    phis[k] = X_k.shape[0] / float(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
